Manipulation de Corpus de Texte et Mod√©lisation de Langage
Objectif du TP
L'objectif de ce TP est de manipuler des corpus de texte en les consid√©rant comme des signaux sous forme de s√©quences de lettres. Le travail se divise en deux grandes parties :

√âtude des liens entre la taille d'un corpus et la taille de son lexique, et v√©rification de la loi de Zipf sur la distribution des mots.
Application de mod√®les de signal sur des signaux discrets, avec une attention particuli√®re √† l'utilisation de cha√Ænes de Markov pour la classification th√©matique de textes.
Environnement de travail
Vous avez le choix entre deux environnements de d√©veloppement pour ce TP :

Environnement Unix avec Bash Shell (disponible par d√©faut dans les distributions Ubuntu).
Environnement Jupyter Notebook, avec un exemple fourni pour le TP1.
Les scripts sont principalement en Python, mais vous √™tes libre d'utiliser d'autres langages. L'efficacit√© des algorithmes est un point essentiel, donc tenez compte de la complexit√© des op√©rations en fonction de la taille des corpus.

Ressources fournies
Les ressources n√©cessaires au TP sont contenues dans le fichier TP1_data.tgz qui comprend :

corpus_topic (2.9 Mo) : Corpus de d√©p√™ches de presse en texte brut, class√©es selon 4 th√©matiques : Culture, √âconomie, International, Sport.
frwiki-sample.txt.gz (56 Mo) : Extrait du dump (ancien) de Wikipedia en fran√ßais.
orfeo_dump.txt.gz (20 Mo) : Corpus de transcription de conversations orales.
D√©but du TP : Tokenization
Le traitement initial consiste √† appliquer une √©tape de tokenization, c'est-√†-dire d√©couper le flux de texte d'un corpus en une s√©quence de "tokens". Un programme Python (tokenize.py) est mis √† votre disposition pour effectuer cette t√¢che en utilisant un algorithme glouton bas√© sur un lexique (lexique_code.txt) contenant 89,365 mots.

Les mots inconnus du lexique sont cod√©s par le code 0.

Partie 1 : Loi de Zipf
Question 1 : Taille du Corpus et Taille du Lexique
Vous devez d√©terminer la relation entre la taille d'un corpus et la taille de son lexique. Pour cela, √©crivez un programme qui calcule la taille du lexique pour diff√©rentes tailles de corpus. Les tailles de corpus √† consid√©rer sont : 100, 500, 1000, 1500, 2000, 4000, 5000, 10000, 20000, 30000, 40000, 50000, 80000, 100000, 200000, 500000, 1000000, 2000000, 5000000.

Exemple de sortie attendue :

python-repl
Copier
Modifier
100 63
500 201
1000 323
1500 406
...
5000000 60182
Question 2 : Comparaison de Distributions
Vous comparerez les distributions taille du corpus / taille du lexique pour les corpus frwiki-sample.txt et orfeo_dump.txt. Utilisez la commande Unix gnuplot pour afficher les courbes, en utilisant un fichier de donn√©es pour chaque corpus.

Exemple de commande :

bash
Copier
Modifier
set grid
set logscale x
set logscale y
set xlabel 'taille corpus'
set ylabel 'taille lexique'
plot 'wiki.data' w lp t '√©crit', 'orfeo.data' w lp t 'oral'
Si vous souhaitez sauvegarder la courbe dans un fichier PDF :

bash
Copier
Modifier
set terminal pdf
set output 'taille_lexique.pdf'
replot
Question 3 : Interpr√©tation des Courbes
Analysez les courbes obtenues et discutez :

Du rapport entre la taille du corpus et la taille du lexique.
Des diff√©rences entre la langue orale (corpus orfeo_dump.txt) et la langue √©crite (corpus frwiki-sample.txt).
Partie 2 : V√©rification de la Loi de Zipf
Question 1 : Comparaison des Courbes
Cr√©ez un fichier contenant les mots class√©s par fr√©quence d√©croissante pour chaque corpus. Pour chaque corpus, g√©n√©rez une courbe rang / fr√©quence en utilisant gnuplot.

Comparez les courbes des corpus Wikipedia et Orfeo, et interpr√©tez les r√©sultats.

Question 2 : Comparaison entre Th√©matiques
Comparez les courbes des corpus des diff√©rentes th√©matiques pr√©sentes dans le r√©pertoire corpus_topic (Culture, √âconomie, International, Sport). Que pouvez-vous d√©duire de ces comparaisons ?

Partie 3 : Cha√Ænes de Markov pour la Classification Th√©matique
Question 1 : Calcul des Probabilit√©s des Bigrammes
√âcrivez un programme qui prend un corpus en entr√©e et calcule les probabilit√©s des bigrammes (paires de mots cons√©cutifs) dans le corpus, en utilisant la formule du maximum de vraisemblance :

ùëÉ
(
ùëä
2
‚à£
ùëä
1
)
=
‚à£
ùëä
1
,
ùëä
2
‚à£
‚à£
ùëä
1
‚à£
P(W2‚à£W1)= 
‚à£W1‚à£
‚à£W1,W2‚à£
‚Äã
 

Stockez ces probabilit√©s dans un fichier JSON, qui servira de mod√®le pour chaque th√®me du corpus.

Question 2 : Calcul de la Log-Probabilit√©
√âcrivez un programme qui prend en entr√©e un mod√®le de bigramme et un corpus tokenis√©, et qui calcule la log-probabilit√© du corpus par rapport au mod√®le. Ignorez les bigrammes qui ne sont pas pr√©sents dans le mod√®le.

Question 3 : Classification Automatique des Th√®mes
Utilisez le programme pr√©c√©dent pour classifier automatiquement le th√®me des fichiers de test. Pour chaque fichier test, calculez les valeurs de log-probabilit√© pour les 4 mod√®les th√©matiques (Culture, √âconomie, International, Sport). Attribuez le th√®me correspondant √† la log-probabilit√© la plus petite.

Question 4 : Mesure du Taux de Bonne Classification
Mesurez le taux de bonne classification pour chaque th√®me en comparant les pr√©dictions de vos mod√®les avec les √©tiquettes r√©elles des corpus de test. Analysez les r√©sultats obtenus.

Question Bonus : Mod√®les Unigramme et Trigramme
R√©p√©tez les exp√©riences pr√©c√©dentes en utilisant d'abord un mod√®le unigramme (probabilit√© d'un mot seul) puis un mod√®le trigramme (probabilit√© d'un mot donn√© les deux mots pr√©c√©dents). Comparez les r√©sultats et discutez des avantages et inconv√©nients de chaque mod√®le.
